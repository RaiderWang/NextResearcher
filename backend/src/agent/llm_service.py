"""LLMæœåŠ¡å±‚

æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£ï¼Œç®€åŒ–graph.pyä¸­çš„LLMä½¿ç”¨ã€‚
"""

import os
from typing import Dict, Any, List, Optional, Type, Union
from langchain_core.language_models import BaseLanguageModel
from langchain_core.runnables import RunnableConfig

# ç¡®ä¿æä¾›å•†æ³¨å†Œ
from . import provider_registry

from .llm_factory import LLMProviderFactory
from .llm_providers import BaseLLMProvider
from .llm_types import (
    LLMRequest,
    LLMResponse,
    LLMProviderType,
    LLMTaskType,
    LLMModel,
    LLMProviderError
)


class LLMService:
    """LLMæœåŠ¡ç±»
    
    æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£ï¼Œç®¡ç†ä¸åŒæä¾›å•†çš„LLMå®ä¾‹ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–LLMæœåŠ¡"""
        self._factory = LLMProviderFactory()
        self._default_provider: Optional[BaseLLMProvider] = None
        self._providers_cache: Dict[str, BaseLLMProvider] = {}
    
    def get_default_provider(self) -> BaseLLMProvider:
        """è·å–é»˜è®¤LLMæä¾›å•†
        
        Returns:
            BaseLLMProvider: é»˜è®¤æä¾›å•†å®ä¾‹
        """
        if self._default_provider is None:
            self._default_provider = self._factory.create_default_provider()
        return self._default_provider
    
    def get_provider(self, provider_type: LLMProviderType) -> BaseLLMProvider:
        """è·å–æŒ‡å®šç±»å‹çš„LLMæä¾›å•†
        
        Args:
            provider_type: æä¾›å•†ç±»å‹
            
        Returns:
            BaseLLMProvider: æä¾›å•†å®ä¾‹
        """
        cache_key = str(provider_type)
        if cache_key not in self._providers_cache:
            self._providers_cache[cache_key] = self._factory.create_provider(provider_type)
        return self._providers_cache[cache_key]
    
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        provider_type: Optional[LLMProviderType] = None,
        task_type: LLMTaskType = LLMTaskType.QUERY_GENERATION,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """ç”Ÿæˆæ–‡æœ¬å†…å®¹
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            model: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            provider_type: æä¾›å•†ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æä¾›å•†
            task_type: ä»»åŠ¡ç±»å‹
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            LLMResponse: ç”Ÿæˆå“åº”
        """
        # è·å–æä¾›å•†
        if provider_type:
            provider = self.get_provider(provider_type)
        else:
            provider = self.get_default_provider()
        
        # ä½¿ç”¨æä¾›å•†çš„é»˜è®¤æ¨¡å‹ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if not model:
            model = provider.get_default_model()
        
        # åˆ›å»ºè¯·æ±‚
        request = LLMRequest(
            prompt=prompt,
            model=model,
            task_type=task_type,
            temperature=temperature,
            max_tokens=max_tokens,
            additional_params=kwargs
        )
        
        return await provider.generate(request)
    
    async def generate_structured(
        self,
        prompt: str,
        output_schema: Type,
        model: Optional[str] = None,
        provider_type: Optional[LLMProviderType] = None,
        task_type: LLMTaskType = LLMTaskType.QUERY_GENERATION,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """ç”Ÿæˆç»“æ„åŒ–è¾“å‡º
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            output_schema: è¾“å‡ºç»“æ„çš„Pydanticæ¨¡å‹ç±»
            model: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            provider_type: æä¾›å•†ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æä¾›å•†
            task_type: ä»»åŠ¡ç±»å‹
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            LLMResponse: åŒ…å«ç»“æ„åŒ–æ•°æ®çš„å“åº”
        """
        # è·å–æä¾›å•†
        if provider_type:
            provider = self.get_provider(provider_type)
        else:
            provider = self.get_default_provider()
        
        # ä½¿ç”¨æä¾›å•†çš„é»˜è®¤æ¨¡å‹ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if not model:
            model = provider.get_default_model()
        
        # åˆ›å»ºè¯·æ±‚
        request = LLMRequest(
            prompt=prompt,
            model=model,
            task_type=task_type,
            temperature=temperature,
            max_tokens=max_tokens,
            structured_output_schema=output_schema.model_json_schema() if hasattr(output_schema, 'model_json_schema') else None,
            additional_params=kwargs
        )
        
        return await provider.generate_structured(request, output_schema)
    
    def get_langchain_llm(
        self,
        model: Optional[str] = None,
        provider_type: Optional[LLMProviderType] = None,
        **kwargs
    ) -> BaseLanguageModel:
        """è·å–LangChainå…¼å®¹çš„LLMå®ä¾‹
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            provider_type: æä¾›å•†ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æä¾›å•†
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            BaseLanguageModel: LangChainå…¼å®¹çš„LLMå®ä¾‹
        """
        # è·å–æä¾›å•†
        if provider_type:
            provider = self.get_provider(provider_type)
        else:
            provider = self.get_default_provider()
        
        # ä½¿ç”¨æä¾›å•†çš„é»˜è®¤æ¨¡å‹ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if not model:
            model = provider.get_default_model()
        
        return provider.get_langchain_llm(model, **kwargs)
    
    async def get_all_available_models(self) -> Dict[LLMProviderType, List[LLMModel]]:
        """è·å–æ‰€æœ‰æä¾›å•†çš„å¯ç”¨æ¨¡å‹
        
        Returns:
            Dict[LLMProviderType, List[LLMModel]]: æŒ‰æä¾›å•†åˆ†ç»„çš„æ¨¡å‹åˆ—è¡¨
        """
        return await self._factory.get_all_available_models()
    
    def get_available_providers(self) -> List[LLMProviderType]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æä¾›å•†ç±»å‹
        
        Returns:
            List[LLMProviderType]: å¯ç”¨æä¾›å•†ç±»å‹åˆ—è¡¨
        """
        return self._factory.get_available_providers()
    
    async def health_check_all(self) -> Dict[LLMProviderType, bool]:
        """æ£€æŸ¥æ‰€æœ‰æä¾›å•†çš„å¥åº·çŠ¶æ€
        
        Returns:
            Dict[LLMProviderType, bool]: å„æä¾›å•†çš„å¥åº·çŠ¶æ€
        """
        return await self._factory.health_check_all()
    
    def get_provider_info(self, provider_type: LLMProviderType) -> Dict[str, Any]:
        """è·å–æä¾›å•†ä¿¡æ¯
        
        Args:
            provider_type: æä¾›å•†ç±»å‹
            
        Returns:
            Dict[str, Any]: æä¾›å•†ä¿¡æ¯
        """
        try:
            provider = self.get_provider(provider_type)
            return {
                "type": provider.get_provider_type(),
                "default_model": provider.get_default_model(),
                "available_models": provider.get_available_model_names(),
                "rate_limits": provider.get_rate_limits(),
                "config_valid": provider.validate_config()
            }
        except Exception as e:
            return {
                "type": provider_type,
                "error": str(e),
                "config_valid": False
            }


class ConfigurableLLMService(LLMService):
    """å¯é…ç½®çš„LLMæœåŠ¡
    
    æ”¯æŒä»RunnableConfigä¸­è¯»å–é…ç½®å‚æ•°ã€‚
    """
    
    def __init__(self, config: Optional[RunnableConfig] = None):
        """åˆå§‹åŒ–å¯é…ç½®LLMæœåŠ¡
        
        Args:
            config: è¿è¡Œæ—¶é…ç½®
        """
        super().__init__()
        self.config = config or {}
        self._extract_config_params()
    
    def _extract_config_params(self):
        """ä»é…ç½®ä¸­æå–å‚æ•°"""
        configurable = self.config.get("configurable", {})
        
        # æ·»åŠ å®Œæ•´çš„é…ç½®è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ”§ ConfigurableLLMService: é…ç½®è§£æ")
        print(f"ğŸ”§ configurable keys: {list(configurable.keys()) if configurable else 'None'}")
        print(f"ğŸ”§ reasoning_model in configurable: {configurable.get('reasoning_model') if configurable else 'N/A'}")
        print(f"ğŸ”§ config top-level keys: {list(self.config.keys()) if self.config else 'None'}")
        print(f"ğŸ”§ reasoning_model in config: {self.config.get('reasoning_model') if self.config else 'N/A'}")
        
        # ä½¿ç”¨ä¸Configurationç›¸åŒçš„å‚æ•°è¯»å–é€»è¾‘
        def get_param_value(param_name: str):
            """è·å–å‚æ•°å€¼ï¼Œä¼˜å…ˆçº§ï¼šconfigurable > é¡¶å±‚config > ç¯å¢ƒå˜é‡"""
            # 1. ä»configurableä¸­è·å–
            value = configurable.get(param_name)
            if value is not None:
                return value
            
            # 2. ä»é¡¶å±‚configä¸­è·å–
            value = self.config.get(param_name)
            if value is not None:
                return value
            
            # 3. ä»ç¯å¢ƒå˜é‡è·å–
            import os
            return os.environ.get(param_name.upper())
        
        # æå–LLMç›¸å…³é…ç½®
        self.query_generator_model = get_param_value("query_generator_model")
        self.reflection_model = get_param_value("reflection_model")
        self.answer_model = get_param_value("answer_model")
        self.llm_provider = get_param_value("llm_provider")
        
        # å¦‚æœç”¨æˆ·è®¾ç½®äº†reasoning_modelï¼Œç”¨å®ƒæ¥è¦†ç›–æ‰€æœ‰ä»»åŠ¡æ¨¡å‹
        reasoning_model = get_param_value("reasoning_model")
        if reasoning_model:
            print(f"ğŸ”§ ç”¨æˆ·é€‰æ‹©äº†reasoning_model: {reasoning_model}ï¼Œå°†ç”¨äºæ‰€æœ‰ä»»åŠ¡")
            self.query_generator_model = reasoning_model
            self.reflection_model = reasoning_model  
            self.answer_model = reasoning_model
        
        print(f"ğŸ”§ æœ€ç»ˆå‚æ•°:")
        print(f"ğŸ”§ - query_generator_model: {self.query_generator_model}")
        print(f"ğŸ”§ - reflection_model: {self.reflection_model}")
        print(f"ğŸ”§ - answer_model: {self.answer_model}")
        print(f"ğŸ”§ - llm_provider: {self.llm_provider}")
        print(f"ğŸ”§ - reasoning_model (ç”¨æˆ·é€‰æ‹©): {reasoning_model}")
        
        # å¦‚æœæŒ‡å®šäº†æä¾›å•†ï¼Œå°è¯•è·å–å¯¹åº”çš„æä¾›å•†å®ä¾‹
        if self.llm_provider:
            try:
                provider_type = LLMProviderType(self.llm_provider)
                self._default_provider = self.get_provider(provider_type)
                print(f"ğŸ”§ æˆåŠŸè®¾ç½®æä¾›å•†: {provider_type}")
            except (ValueError, LLMProviderError) as e:
                print(f"Warning: Failed to set provider {self.llm_provider}: {e}")
        else:
            print(f"ğŸ”§ æœªæŒ‡å®šllm_providerï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤æä¾›å•†")
    
    def get_model_for_task(self, task_type: LLMTaskType) -> Optional[str]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å¯¹åº”çš„æ¨¡å‹
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            Optional[str]: æ¨¡å‹åç§°
        """
        if task_type == LLMTaskType.QUERY_GENERATION:
            return self.query_generator_model
        elif task_type == LLMTaskType.REFLECTION:
            return self.reflection_model
        elif task_type == LLMTaskType.ANSWER_GENERATION:
            return self.answer_model
        else:
            return None
    
    async def generate_for_task(
        self,
        prompt: str,
        task_type: LLMTaskType,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆå†…å®¹
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            task_type: ä»»åŠ¡ç±»å‹
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            LLMResponse: ç”Ÿæˆå“åº”
        """
        model = self.get_model_for_task(task_type)
        return await self.generate(
            prompt=prompt,
            model=model,
            task_type=task_type,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
    
    async def generate_structured_for_task(
        self,
        prompt: str,
        output_schema: Type,
        task_type: LLMTaskType,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆç»“æ„åŒ–å†…å®¹
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            output_schema: è¾“å‡ºç»“æ„çš„Pydanticæ¨¡å‹ç±»
            task_type: ä»»åŠ¡ç±»å‹
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            LLMResponse: åŒ…å«ç»“æ„åŒ–æ•°æ®çš„å“åº”
        """
        model = self.get_model_for_task(task_type)
        return await self.generate_structured(
            prompt=prompt,
            output_schema=output_schema,
            model=model,
            task_type=task_type,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
    
    def get_default_provider(self) -> BaseLLMProvider:
        """è·å–é»˜è®¤LLMæä¾›å•†å®ä¾‹
        
        é‡å†™çˆ¶ç±»æ–¹æ³•ï¼Œä¼˜å…ˆä½¿ç”¨é…ç½®æŒ‡å®šçš„æä¾›å•†
        
        Returns:
            BaseLLMProvider: é»˜è®¤æä¾›å•†å®ä¾‹
        """
        # å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†æä¾›å•†ï¼Œä½¿ç”¨é…ç½®çš„æä¾›å•†
        if hasattr(self, '_default_provider') and self._default_provider is not None:
            print(f"ğŸ”§ get_default_provider: ä½¿ç”¨é…ç½®çš„æä¾›å•†: {self._default_provider.get_provider_type()}")
            return self._default_provider
        
        # å¦åˆ™ä½¿ç”¨çˆ¶ç±»çš„é»˜è®¤æä¾›å•†
        print(f"ğŸ”§ get_default_provider: ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æä¾›å•†")
        return super().get_default_provider()


# å…¨å±€LLMæœåŠ¡å®ä¾‹
_llm_service: Optional[LLMService] = None


def get_llm_service() -> LLMService:
    """è·å–å…¨å±€LLMæœåŠ¡å®ä¾‹
    
    Returns:
        LLMService: LLMæœåŠ¡å®ä¾‹
    """
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service


def create_configurable_llm_service(config: RunnableConfig) -> ConfigurableLLMService:
    """åˆ›å»ºå¯é…ç½®çš„LLMæœåŠ¡å®ä¾‹
    
    Args:
        config: è¿è¡Œæ—¶é…ç½®
        
    Returns:
        ConfigurableLLMService: å¯é…ç½®LLMæœåŠ¡å®ä¾‹
    """
    return ConfigurableLLMService(config)