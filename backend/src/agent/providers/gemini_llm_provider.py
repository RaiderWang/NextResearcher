"""Google Gemini LLMæä¾›å•†å®ç°

åŸºäºç°æœ‰çš„Geminié›†æˆï¼Œæä¾›æ ‡å‡†åŒ–çš„LLMæ¥å£ã€‚
"""

import os
from typing import Dict, Any, List, Optional, Type
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.language_models import BaseLanguageModel
from google.genai import Client

from ..llm_providers import BaseLLMProvider, LLMProviderRegistry
from ..llm_types import (
    LLMRequest,
    LLMResponse,
    LLMProviderType,
    LLMModel,
    GeminiProviderConfig,
    LLMProviderError,
    LLMProviderAPIError,
    LLMProviderTimeoutError
)


class GeminiLLMProvider(BaseLLMProvider):
    """Google Gemini LLMæä¾›å•†
    
    å°è£…Google Gemini APIçš„è°ƒç”¨ï¼Œæä¾›æ ‡å‡†åŒ–æ¥å£ã€‚
    """
    
    def __init__(self, config: GeminiProviderConfig):
        """åˆå§‹åŒ–Geminiæä¾›å•†
        
        Args:
            config: Geminié…ç½®
        """
        super().__init__(config)
        self.config: GeminiProviderConfig = config
        self._genai_client = Client(api_key=config.api_key)
        
        # é¢„å®šä¹‰çš„æ¨¡å‹ä¿¡æ¯
        self._model_info = {
            "gemini-2.0-flash": LLMModel(
                id="gemini-2.0-flash",
                name="Gemini 2.0 Flash",
                provider=LLMProviderType.GEMINI,
                max_tokens=8192,
                supports_structured_output=True,
                description="Fast and efficient model for general tasks"
            ),
            "gemini-2.5-flash": LLMModel(
                id="gemini-2.5-flash",
                name="Gemini 2.5 Flash",
                provider=LLMProviderType.GEMINI,
                max_tokens=8192,
                supports_structured_output=True,
                description="Enhanced flash model with improved capabilities"
            ),
            "gemini-2.5-pro": LLMModel(
                id="gemini-2.5-pro",
                name="Gemini 2.5 Pro",
                provider=LLMProviderType.GEMINI,
                max_tokens=32768,
                supports_structured_output=True,
                description="Advanced model for complex reasoning tasks"
            )
        }
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ç”Ÿæˆæ–‡æœ¬å†…å®¹
        
        Args:
            request: æ ‡å‡†åŒ–çš„LLMè¯·æ±‚
            
        Returns:
            LLMResponse: æ ‡å‡†åŒ–çš„LLMå“åº”
        """
        await self._rate_limit_check()
        request = self._prepare_request(request)
        
        try:
            # åˆ›å»ºLangChain LLMå®ä¾‹
            llm = self.get_langchain_llm(
                model=request.model,
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )
            
            # è°ƒç”¨LLM
            result = llm.invoke(request.prompt)
            
            # æå–å†…å®¹
            content = result.content if hasattr(result, 'content') else str(result)
            
            return self._create_response(
                content=content,
                model=request.model,
                metadata={
                    "task_type": request.task_type,
                    "temperature": request.temperature,
                    "max_tokens": request.max_tokens
                }
            )
            
        except Exception as e:
            raise LLMProviderAPIError(f"Gemini API error: {str(e)}")
    
    async def generate_structured(self, request: LLMRequest, output_schema: Type) -> LLMResponse:
        """ç”Ÿæˆç»“æ„åŒ–è¾“å‡º
        
        Args:
            request: æ ‡å‡†åŒ–çš„LLMè¯·æ±‚
            output_schema: è¾“å‡ºç»“æ„çš„Pydanticæ¨¡å‹ç±»
            
        Returns:
            LLMResponse: åŒ…å«ç»“æ„åŒ–æ•°æ®çš„å“åº”
        """
        await self._rate_limit_check()
        request = self._prepare_request(request)
        
        try:
            print(f"ğŸ”§ Gemini structured_output - å¼€å§‹ç”Ÿæˆ")
            print(f"ğŸ”§ - æ¨¡å‹: {request.model}")
            print(f"ğŸ”§ - schema: {output_schema.__name__}")
            print(f"ğŸ”§ - prompté•¿åº¦: {len(request.prompt)}")
            
            # åˆ›å»ºæ”¯æŒç»“æ„åŒ–è¾“å‡ºçš„LLMå®ä¾‹
            llm = self.get_langchain_llm(
                model=request.model,
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )
            
            # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            structured_llm = llm.with_structured_output(output_schema)
            print(f"ğŸ”§ Gemini - è°ƒç”¨structured_llm.invoke")
            
            try:
                result = structured_llm.invoke(request.prompt)
                print(f"ğŸ”§ Gemini - ç»“æ„åŒ–è¾“å‡ºç»“æœ: {result}")
                print(f"ğŸ”§ Gemini - ç»“æœç±»å‹: {type(result)}")
                
                # å¦‚æœç»“æœä¸ºNoneï¼Œå°è¯•ä½¿ç”¨æ™®é€šç”Ÿæˆç„¶åè§£æ
                if result is None:
                    print(f"âš ï¸ Gemini - ç»“æ„åŒ–è¾“å‡ºä¸ºNoneï¼Œå°è¯•æ™®é€šç”Ÿæˆ")
                    plain_result = llm.invoke(request.prompt)
                    print(f"ğŸ”§ Gemini - æ™®é€šç”Ÿæˆç»“æœ: {plain_result}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯MAX_TOKENSé—®é¢˜
                    if (hasattr(plain_result, 'response_metadata') and 
                        plain_result.response_metadata.get('finish_reason') == 'MAX_TOKENS'):
                        print(f"âš ï¸ Gemini - æ£€æµ‹åˆ°MAX_TOKENSé—®é¢˜ï¼Œä½¿ç”¨æ›´å¤§çš„tokené™åˆ¶é‡è¯•")
                        
                        # å¢åŠ tokené™åˆ¶é‡è¯•
                        retry_llm = self.get_langchain_llm(
                            model=request.model,
                            temperature=request.temperature,
                            max_tokens=min(request.max_tokens * 2, 8000)  # ç¿»å€ä½†ä¸è¶…è¿‡8000
                        )
                        retry_structured_llm = retry_llm.with_structured_output(output_schema)
                        result = retry_structured_llm.invoke(request.prompt)
                        print(f"ğŸ”§ Gemini - é‡è¯•ç»“æœ: {result}")
                    
                    # å¦‚æœä»ç„¶ä¸ºNoneï¼Œå°è¯•æ‰‹åŠ¨è§£æ
                    if result is None:
                        # å°è¯•è§£æä¸ºç»“æ„åŒ–æ•°æ®
                        try:
                            import json
                            content_text = plain_result.content if hasattr(plain_result, 'content') else str(plain_result)
                            # ç®€å•çš„JSONè§£æå°è¯•
                            if content_text.strip().startswith('{'):
                                parsed_data = json.loads(content_text.strip())
                                result = output_schema(**parsed_data)
                                print(f"ğŸ”§ Gemini - æ‰‹åŠ¨è§£ææˆåŠŸ: {result}")
                        except Exception as parse_error:
                            print(f"âš ï¸ Gemini - æ‰‹åŠ¨è§£æå¤±è´¥: {parse_error}")
                            # ä½¿ç”¨åŸå§‹æ–‡æœ¬ä½œä¸ºcontentï¼Œä½†structured_dataä¸ºNone
                            content = content_text
                            result = None
                
                # ç”Ÿæˆæ–‡æœ¬è¡¨ç¤º
                content = str(result) if result else ""
                print(f"ğŸ”§ Gemini - ç”Ÿæˆçš„content: '{content}'")
                
            except Exception as invoke_error:
                print(f"âŒ Gemini - invokeè°ƒç”¨å¤±è´¥: {invoke_error}")
                raise invoke_error
            
            return self._create_response(
                content=content,
                model=request.model,
                structured_data=result,
                metadata={
                    "task_type": request.task_type,
                    "temperature": request.temperature,
                    "max_tokens": request.max_tokens,
                    "structured_output": True,
                    "schema": output_schema.__name__
                }
            )
            
        except Exception as e:
            raise LLMProviderAPIError(f"Gemini structured output error: {str(e)}")
    
    def get_langchain_llm(self, model: str, **kwargs) -> BaseLanguageModel:
        """è·å–LangChainå…¼å®¹çš„LLMå®ä¾‹
        
        Args:
            model: æ¨¡å‹åç§°
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            BaseLanguageModel: LangChainå…¼å®¹çš„LLMå®ä¾‹
        """
        return ChatGoogleGenerativeAI(
            model=model,
            api_key=self.config.api_key,
            max_retries=self.config.max_retries,
            timeout=self.config.timeout,
            **kwargs
        )
    
    def get_provider_type(self) -> LLMProviderType:
        """è¿”å›æä¾›å•†ç±»å‹"""
        return LLMProviderType.GEMINI
    
    def validate_config(self) -> bool:
        """éªŒè¯æä¾›å•†é…ç½®æ˜¯å¦æœ‰æ•ˆ"""
        try:
            # æ£€æŸ¥APIå¯†é’¥
            if not self.config.api_key:
                return False
            
            # æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
            if not self.config.models:
                return False
            
            # æ£€æŸ¥é»˜è®¤æ¨¡å‹æ˜¯å¦åœ¨åˆ—è¡¨ä¸­
            if self.config.default_model not in self.config.models:
                return False
            
            return True
            
        except Exception:
            return False
    
    async def get_available_models(self) -> List[LLMModel]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        models = []
        for model_id in self.config.models:
            if model_id in self._model_info:
                models.append(self._model_info[model_id])
            else:
                # ä¸ºæœªçŸ¥æ¨¡å‹åˆ›å»ºåŸºæœ¬ä¿¡æ¯
                models.append(LLMModel(
                    id=model_id,
                    name=model_id,
                    provider=LLMProviderType.GEMINI,
                    supports_structured_output=True,
                    description=f"Gemini model: {model_id}"
                ))
        
        return models
    
    def get_rate_limits(self) -> Dict[str, Any]:
        """è·å–APIé€Ÿç‡é™åˆ¶ä¿¡æ¯"""
        return {
            "requests_per_minute": 60,
            "requests_per_second": 1,
            "tokens_per_minute": 32000,
            "concurrent_requests": 5
        }
    
    async def generate_with_google_search(self, prompt: str, model: str) -> tuple[str, List[Dict[str, str]]]:
        """ä½¿ç”¨Googleæœç´¢åŠŸèƒ½ç”Ÿæˆå†…å®¹ï¼ˆä¿æŒä¸åŸæœ‰ä»£ç å…¼å®¹ï¼‰
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            model: ä½¿ç”¨çš„æ¨¡å‹
            
        Returns:
            tuple[str, List[Dict[str, str]]]: (ç”Ÿæˆçš„å†…å®¹, æœç´¢æ¥æºåˆ—è¡¨)
        """
        try:
            # ä½¿ç”¨Google GenAIå®¢æˆ·ç«¯è¿›è¡Œæœç´¢å¢å¼ºç”Ÿæˆ
            response = self._genai_client.models.generate_content(
                model=model,
                contents=prompt,
                config={
                    "tools": [{"google_search": {}}],
                    "temperature": 0,
                }
            )
            
            # æå–å†…å®¹å’Œæ¥æº
            content = response.text if response.text else ""
            sources = []
            
            # å¤„ç†grounding metadataï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                    grounding_chunks = candidate.grounding_metadata.grounding_chunks
                    for i, chunk in enumerate(grounding_chunks):
                        if hasattr(chunk, 'web') and chunk.web:
                            sources.append({
                                "label": f"[{i+1}]",
                                "url": chunk.web.uri,
                                "title": chunk.web.title or "Unknown Title"
                            })
            
            return content, sources
            
        except Exception as e:
            raise LLMProviderAPIError(f"Gemini Google Search error: {str(e)}")


# æ³¨å†ŒGeminiæä¾›å•†
LLMProviderRegistry.register(LLMProviderType.GEMINI, GeminiLLMProvider)