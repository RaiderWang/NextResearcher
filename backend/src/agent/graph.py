import os

from agent.tools_and_schemas import SearchQueryList, Reflection
from dotenv import load_dotenv
from langchain_core.messages import AIMessage
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from google.genai import Client

# å¯¼å…¥æœç´¢æä¾›å•†ç›¸å…³æ¨¡å—
from agent.search_factory import SearchProviderFactory
from agent.search_providers import SearchRequest

# å¯¼å…¥LLMæœåŠ¡ç›¸å…³æ¨¡å—
# ç¡®ä¿æä¾›å•†æ³¨å†Œ
from agent import provider_registry
from agent.llm_service import get_llm_service, ConfigurableLLMService
from agent.llm_types import LLMRequest, LLMTaskType

from agent.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
)
from agent.configuration import Configuration
from agent.prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
)
from agent.utils import (
    get_research_topic,
)

# åŠ è½½ç¯å¢ƒå˜é‡ - ä»backend/.envæ–‡ä»¶è¯»å–
backend_dir = os.path.join(os.path.dirname(__file__), '..', '..')
env_path = os.path.join(backend_dir, '.env')
load_dotenv(dotenv_path=env_path)

if os.getenv("GEMINI_API_KEY") is None:
    raise ValueError("GEMINI_API_KEY is not set")

# Used for Google Search API
genai_client = Client(api_key=os.getenv("GEMINI_API_KEY"))


# Nodes
async def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """LangGraph node that generates search queries based on the User's question.

    Uses the configured LLM provider to create optimized search queries for web research based on
    the User's question.

    Args:
        state: Current graph state containing the User's question
        config: Configuration for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including search_query key containing the generated queries
    """
    configurable = Configuration.from_runnable_config(config)

    # check for custom initial search query count
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries

    try:
        # æ·»åŠ è¯¦ç»†çš„çŠ¶æ€è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ› DEBUG: å½“å‰çŠ¶æ€çš„æ‰€æœ‰é”®: {list(state.keys())}")
        print(f"ğŸ› DEBUG: çŠ¶æ€ä¸­çš„llm_providerå€¼: {state.get('llm_provider')}")
        print(f"ğŸ› DEBUG: çŠ¶æ€ä¸­çš„reasoning_modelå€¼: {state.get('reasoning_model')}")
        
        # ä»çŠ¶æ€ä¸­è·å–ç”¨æˆ·é€‰æ‹©çš„å‚æ•°
        llm_provider_from_state = state.get("llm_provider")
        reasoning_model_from_state = state.get("reasoning_model")
        
        # åˆ›å»ºä¿®æ”¹è¿‡çš„configæ¥ä¼ é€’çŠ¶æ€å‚æ•°ç»™ConfigurableLLMService
        modified_config = config.copy() if config else {}
        if "configurable" not in modified_config:
            modified_config["configurable"] = {}
        
        # ä¼ é€’llm_provider
        if llm_provider_from_state and llm_provider_from_state.strip():
            modified_config["configurable"]["llm_provider"] = llm_provider_from_state
            print(f"ğŸ”§ generate_query - ä»çŠ¶æ€è·å–llm_provider: {llm_provider_from_state}")
        
        # ä¼ é€’reasoning_model
        if reasoning_model_from_state and reasoning_model_from_state.strip():
            modified_config["configurable"]["reasoning_model"] = reasoning_model_from_state
            print(f"ğŸ”§ generate_query - ä»çŠ¶æ€è·å–reasoning_model: {reasoning_model_from_state}")
        
        llm_service = ConfigurableLLMService(modified_config)
        
        # Format the prompt
        current_date = get_current_date()
        formatted_prompt = query_writer_instructions.format(
            current_date=current_date,
            research_topic=get_research_topic(state["messages"]),
            number_queries=state["initial_search_query_count"],
        )
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"ğŸ” generate_query: å‡†å¤‡è°ƒç”¨LLMæœåŠ¡")
        print(f"ğŸ” formatted_prompté•¿åº¦: {len(formatted_prompt)}")
        
        # ä»stateä¸­è·å–ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        model = state.get("reasoning_model", configurable.query_generator_model)
        print(f"ğŸ” generate_query - ä½¿ç”¨æ¨¡å‹: {model}")
        print(f"ğŸ” generate_query - æ¨¡å‹æ¥æº: {'state' if state.get('reasoning_model') else 'config'}")
        
        # ç”Ÿæˆæœç´¢æŸ¥è¯¢ - ä½¿ç”¨æ­£ç¡®çš„LLMServiceæ¥å£
        result = await llm_service.generate_structured(
            prompt=formatted_prompt,
            output_schema=SearchQueryList,
            model=model,
            task_type=LLMTaskType.QUERY_GENERATION,
            temperature=0.7,  # é™ä½temperatureæé«˜ä¸€è‡´æ€§
            max_tokens=2000   # å¢åŠ tokené™åˆ¶
        )
        
        # éªŒè¯ç»“æœ
        if not result or not hasattr(result, 'query') or not result.query:
            raise ValueError("Failed to generate valid search queries")
            
        return {"search_query": result.query}
    except Exception as e:
        print(f"Error in generate_query: {e}")
        # è¿”å›é™çº§æœç´¢æŸ¥è¯¢
        research_topic = get_research_topic(state["messages"])
        fallback_query = [research_topic] if research_topic else ["general search"]
        return {"search_query": fallback_query}


def continue_to_web_research(state: OverallState):
    """LangGraph node that sends the search queries to the web research node.

    This is used to spawn n number of web research nodes, one for each search query.
    """
    # å®‰å…¨è®¿é—® search_query
    search_queries = state.get("search_query", [])
    if not search_queries:
        # å¦‚æœæ²¡æœ‰æœç´¢æŸ¥è¯¢ï¼Œæä¾›é™çº§æ–¹æ¡ˆ
        search_queries = ["general search"]
    
    search_provider = state.get("search_provider", "google")
    
    return [
        Send("web_research", {
            "search_query": search_query, 
            "id": int(idx),
            "search_provider": search_provider
        })
        for idx, search_query in enumerate(search_queries)
    ]


async def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph node that performs web research using configurable search providers.

    Executes a web search using the configured search provider (Google, Tavily, etc.).

    Args:
        state: Current graph state containing the search query and research loop count
        config: Configuration for the runnable, including search provider settings

    Returns:
        Dictionary with state update, including sources_gathered, research_loop_count, and web_research_results
    """
    # è·å–é…ç½®
    configurable = Configuration.from_runnable_config(config)
    
    # ä»çŠ¶æ€ä¸­è·å–æœç´¢æä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    search_provider_name = state.get("search_provider", configurable.search_provider)
    
    try:
        # åˆ›å»ºæœç´¢æä¾›å•†å®ä¾‹
        print(f"ğŸ” ä½¿ç”¨æœç´¢æä¾›å•†: {search_provider_name}")
        search_provider = SearchProviderFactory.create_provider(
            search_provider_name
        )
        
        # åˆ›å»ºæœç´¢è¯·æ±‚
        search_request = SearchRequest(
            query=state["search_query"],
            max_results=configurable.search_results_limit,
            language="zh-CN"
        )
        
        # æ‰§è¡Œæœç´¢
        search_result = await search_provider.search(search_request)
        
        # å°†æœç´¢ç»“æœè½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
        sources_gathered = []
        for i, source in enumerate(search_result.sources):
            # ç”Ÿæˆç®€çŸ­URLæ ‡è¯†ç¬¦
            short_url = f"[{i+1}]"
            real_url = source.get("url", "")
            title = source.get("title", "")
            
            # æ·»åŠ è¯¦ç»†æ—¥å¿—
            print(f"ğŸ”— å¤„ç†æ¥æº{i+1}: URL={real_url[:100]}..." if len(real_url) > 100 else f"ğŸ”— å¤„ç†æ¥æº{i+1}: URL={real_url}")
            print(f"ğŸ“– æ ‡é¢˜: {title}")
            
            sources_gathered.append({
                "label": short_url,
                "short_url": short_url,
                "value": real_url,
                "title": title
            })
        
        print(f"âœ… {search_provider_name}æœç´¢å®Œæˆ: è·å¾—{len(sources_gathered)}ä¸ªæ¥æºï¼Œå†…å®¹é•¿åº¦{len(search_result.content)}")
        
        # æ·»åŠ sourcesè¯¦ç»†æ—¥å¿—
        for source in sources_gathered:
            print(f"ğŸ“š æ¥æºè¯¦ç»†ä¿¡æ¯: {source['short_url']} -> {source['value'][:50]}...")
            print(f"    æ ‡é¢˜: {source['title']}")
        
        return {
            "sources_gathered": sources_gathered,
            "search_query": [state.get("search_query", "unknown query")],
            "web_research_result": [search_result.content],
        }
        
    except Exception as e:
                 # å¦‚æœæœç´¢æä¾›å•†å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹çš„Googleæœç´¢æ–¹æ³•
         print(f"Search provider failed, falling back to Google Search: {e}")
         return await _fallback_google_search(state, config)


async def _fallback_google_search(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """åŸå§‹çš„Googleæœç´¢å®ç°ï¼Œç”¨ä½œåå¤‡æ–¹æ¡ˆ"""
    # Configure
    configurable = Configuration.from_runnable_config(config)
    formatted_prompt = web_searcher_instructions.format(
        current_date=get_current_date(),
        research_topic=state["search_query"],
    )

    # Uses the google genai client as the langchain client doesn't return grounding metadata
    response = genai_client.models.generate_content(
        model=configurable.query_generator_model,
        contents=formatted_prompt,
        config={
            "tools": [{"google_search": {}}],
            "temperature": 0,
        },
    )
    # ç›´æ¥ä»grounding chunksè·å–çœŸå®URLï¼Œä¸ä½¿ç”¨resolve_urlsè½¬æ¢
    grounding_chunks = response.candidates[0].grounding_metadata.grounding_chunks
    
    # æ„å»ºæ¥æºåˆ—è¡¨ - ç›´æ¥ä½¿ç”¨çœŸå®URL
    sources_gathered = []
    for i, chunk in enumerate(grounding_chunks):
        if hasattr(chunk, 'web') and hasattr(chunk.web, 'uri'):
            real_url = chunk.web.uri
            title = getattr(chunk.web, 'title', f"æœç´¢ç»“æœ{i+1}")
            sources_gathered.append({
                "label": f"æ¥æº{i+1}",
                "short_url": f"[{i+1}]",  # ç®€å•çš„æ ‡è®°
                "value": real_url,  # ä½¿ç”¨çœŸå®URL
                "title": title
            })
    
    # æ¸…ç†Gemini APIè‡ªåŠ¨ç”Ÿæˆçš„å‡é“¾æ¥
    import re
    
    # ç§»é™¤æ‰€æœ‰å½¢å¦‚ [label](https://vertexaisearch.cloud.google.com/...) çš„å‡é“¾æ¥
    pattern = r'\[([^\]]+)\]\(https://vertexaisearch\.cloud\.google\.com/[^)]+\)'
    modified_text = re.sub(pattern, r'\1', response.text)
    
    # ç§»é™¤å•ç‹¬çš„vertexaisearché“¾æ¥
    pattern2 = r'https://vertexaisearch\.cloud\.google\.com/[^\s\])]+'
    modified_text = re.sub(pattern2, '', modified_text).strip()

    return {
        "sources_gathered": sources_gathered,
        "search_query": [state.get("search_query", "unknown query")],
        "web_research_result": [modified_text],
    }


async def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """LangGraph node that identifies knowledge gaps and generates potential follow-up queries.

    Analyzes the current summary to identify areas for further research and generates
    potential follow-up queries. Uses structured output to extract
    the follow-up query in JSON format.

    Args:
        state: Current graph state containing the running summary and research topic
        config: Configuration for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including search_query key containing the generated follow-up query
    """
    configurable = Configuration.from_runnable_config(config)
    # Increment the research loop count and get the reasoning model
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    reasoning_model = state.get("reasoning_model", configurable.reflection_model)

    try:
        # ä»çŠ¶æ€ä¸­è·å–ç”¨æˆ·é€‰æ‹©çš„å‚æ•°
        llm_provider_from_state = state.get("llm_provider")
        reasoning_model_from_state = state.get("reasoning_model")
        
        # åˆ›å»ºä¿®æ”¹è¿‡çš„configæ¥ä¼ é€’çŠ¶æ€å‚æ•°ç»™ConfigurableLLMService
        modified_config = config.copy() if config else {}
        if "configurable" not in modified_config:
            modified_config["configurable"] = {}
        
        # ä¼ é€’llm_provider
        if llm_provider_from_state and llm_provider_from_state.strip():
            modified_config["configurable"]["llm_provider"] = llm_provider_from_state
            print(f"ğŸ”§ reflection - ä»çŠ¶æ€è·å–llm_provider: {llm_provider_from_state}")
        
        # ä¼ é€’reasoning_model
        if reasoning_model_from_state and reasoning_model_from_state.strip():
            modified_config["configurable"]["reasoning_model"] = reasoning_model_from_state
            print(f"ğŸ”§ reflection - ä»çŠ¶æ€è·å–reasoning_model: {reasoning_model_from_state}")
        
        llm_service = ConfigurableLLMService(modified_config)
        
        # Format the prompt
        current_date = get_current_date()
        formatted_prompt = reflection_instructions.format(
            current_date=current_date,
            research_topic=get_research_topic(state.get("messages", [])),
            summaries="\n\n---\n\n".join(state.get("web_research_result", [])),
        )
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"ğŸ¤” reflection: å‡†å¤‡è°ƒç”¨LLMæœåŠ¡")
        print(f"ğŸ¤” formatted_prompté•¿åº¦: {len(formatted_prompt)}")
        print(f"ğŸ¤” ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
        
        # ç”Ÿæˆåæ€ç»“æœ - ä½¿ç”¨æ­£ç¡®çš„LLMServiceæ¥å£ï¼Œå¢åŠ tokené™åˆ¶
        result = await llm_service.generate_structured(
            prompt=formatted_prompt,
            output_schema=Reflection,
            model=reasoning_model,
            task_type=LLMTaskType.REFLECTION,
            temperature=0.7,  # é™ä½temperatureæé«˜ä¸€è‡´æ€§
            max_tokens=4000   # å¤§å¹…å¢åŠ tokené™åˆ¶
        )
        
        print(f"ğŸ¤” åæ€ç»“æœå¯¹è±¡: {result}")
        print(f"ğŸ¤” åæ€ç»“æœç±»å‹: {type(result)}")
        
        # æ­£ç¡®æå–ç»“æ„åŒ–æ•°æ®
        structured_data = result.structured_data if result else None
        
        if structured_data:
            print(f"ğŸ¤” structured_data: {structured_data}")
            print(f"ğŸ¤” structured_data ç±»å‹: {type(structured_data)}")
            print(f"ğŸ¤” is_sufficient: {getattr(structured_data, 'is_sufficient', 'MISSING')}")
            print(f"ğŸ¤” knowledge_gap: {getattr(structured_data, 'knowledge_gap', 'MISSING')}")
            print(f"ğŸ¤” follow_up_queries: {getattr(structured_data, 'follow_up_queries', 'MISSING')}")
            print(f"ğŸ¤” follow_up_queries ç±»å‹: {type(getattr(structured_data, 'follow_up_queries', None))}")
        else:
            print("ğŸ¤” æ²¡æœ‰æ‰¾åˆ° structured_data")
        
        follow_up_queries = getattr(structured_data, 'follow_up_queries', []) if structured_data else []
        print(f"â“ å¤„ç†åçš„åç»­æŸ¥è¯¢: {follow_up_queries}")

        return {
            "is_sufficient": getattr(structured_data, 'is_sufficient', False) if structured_data else False,
            "knowledge_gap": getattr(structured_data, 'knowledge_gap', "") if structured_data else "",
            "follow_up_queries": follow_up_queries,
            "research_loop_count": state.get("research_loop_count", 0),
            "number_of_ran_queries": len(state.get("search_query", [])),
        }
    except Exception as e:
        print(f"Error in reflection: {e}")
        # è¿”å›é»˜è®¤å€¼ï¼Œè¡¨ç¤ºç ”ç©¶å……åˆ†
        return {
            "is_sufficient": True,
            "knowledge_gap": "",
            "follow_up_queries": [],
            "research_loop_count": state.get("research_loop_count", 0),
            "number_of_ran_queries": len(state.get("search_query", [])),
        }


def evaluate_research(
    state: OverallState,
    config: RunnableConfig,
) -> OverallState:
    """LangGraph routing function that determines the next step in the research flow.

    Controls the research loop by deciding whether to continue gathering information
    or to finalize the summary based on the configured maximum number of research loops.

    Args:
        state: Current graph state containing the research loop count
        config: Configuration for the runnable, including max_research_loops setting

    Returns:
        String literal indicating the next node to visit ("web_research" or "finalize_summary")
    """
    configurable = Configuration.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )
    is_sufficient = state.get("is_sufficient", False)
    research_count = state.get("research_loop_count", 0)
    follow_up_queries = state.get("follow_up_queries", [])
    print(f"ğŸ”„ è¯„ä¼°ç ”ç©¶çŠ¶æ€: is_sufficient={is_sufficient}, research_count={research_count}, max_loops={max_research_loops}")
    print(f"ğŸ”„ çŠ¶æ€ä¸­çš„æ‰€æœ‰é”®: {list(state.keys())}")
    print(f"ğŸ”„ ä»çŠ¶æ€è·å–çš„follow_up_queries: {follow_up_queries}")
    
    if is_sufficient or research_count >= max_research_loops:
        print("âœ… ç ”ç©¶å……åˆ†ï¼Œå‰å¾€ finalize_answer")
        return "finalize_answer"
    else:
        print("ğŸ”„ ç»§ç»­ç ”ç©¶ï¼Œç”Ÿæˆæ›´å¤šæŸ¥è¯¢")
        print(f"ğŸ“ ç”Ÿæˆçš„åç»­æŸ¥è¯¢: {follow_up_queries}")
        
        # å¦‚æœæ²¡æœ‰åç»­æŸ¥è¯¢ä½†è¿˜éœ€è¦ç ”ç©¶ï¼Œå¼ºåˆ¶ç»“æŸå¹¶ç”Ÿæˆç­”æ¡ˆ
        if not follow_up_queries:
            print("âš ï¸ æ²¡æœ‰åç»­æŸ¥è¯¢ä½†ç ”ç©¶ä¸å……åˆ†ï¼Œå¼ºåˆ¶å‰å¾€ finalize_answer")
            return "finalize_answer"
        
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state.get("number_of_ran_queries", 0) + int(idx),
                    "search_provider": state.get("search_provider", "google"),
                },
            )
            for idx, follow_up_query in enumerate(follow_up_queries)
        ]


async def finalize_answer(state: OverallState, config: RunnableConfig):
    """LangGraph node that finalizes the research summary.

    Prepares the final output by deduplicating and formatting sources, then
    combining them with the running summary to create a well-structured
    research report with proper citations.

    Args:
        state: Current graph state containing the running summary and sources gathered

    Returns:
        Dictionary with state update, including running_summary key containing the formatted final summary with sources
    """
    print("ğŸ”¥ finalize_answer å‡½æ•°è¢«è°ƒç”¨")
    print(f"ğŸ“Š çŠ¶æ€ä¿¡æ¯: web_research_resultæ•°é‡={len(state.get('web_research_result', []))}")
    print(f"ğŸ“š sources_gatheredæ•°é‡={len(state.get('sources_gathered', []))}")
    
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {reasoning_model}")

    try:
        # ä»çŠ¶æ€ä¸­è·å–ç”¨æˆ·é€‰æ‹©çš„å‚æ•°
        llm_provider_from_state = state.get("llm_provider")
        reasoning_model_from_state = state.get("reasoning_model")
        
        # åˆ›å»ºä¿®æ”¹è¿‡çš„configæ¥ä¼ é€’çŠ¶æ€å‚æ•°ç»™ConfigurableLLMService
        modified_config = config.copy() if config else {}
        if "configurable" not in modified_config:
            modified_config["configurable"] = {}
        
        # ä¼ é€’llm_provider
        if llm_provider_from_state and llm_provider_from_state.strip():
            modified_config["configurable"]["llm_provider"] = llm_provider_from_state
            print(f"ğŸ”§ finalize_answer - ä»çŠ¶æ€è·å–llm_provider: {llm_provider_from_state}")
        
        # ä¼ é€’reasoning_model
        if reasoning_model_from_state and reasoning_model_from_state.strip():
            modified_config["configurable"]["reasoning_model"] = reasoning_model_from_state
            print(f"ğŸ”§ finalize_answer - ä»çŠ¶æ€è·å–reasoning_model: {reasoning_model_from_state}")
        
        llm_service = ConfigurableLLMService(modified_config)
        
        # Format the prompt with enhanced summaries including source information
        current_date = get_current_date()
        research_results = state.get("web_research_result", [])
        all_sources = state.get("sources_gathered", [])
        
        # åˆ›å»ºå¢å¼ºçš„summariesï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        enhanced_summaries = []
        for i, result in enumerate(research_results):
            enhanced_summary = f"## Research Result {i+1}\n\n{result}"
            
            # æ·»åŠ ç›¸å…³æ¥æºä¿¡æ¯
            if all_sources:
                # ä¸ºæ¯ä¸ªresearch resultæ·»åŠ å¯¹åº”çš„æ¥æºä¿¡æ¯
                sources_for_this_result = []
                start_idx = i * 5  # å‡è®¾æ¯ä¸ªç ”ç©¶ç»“æœå¯¹åº”5ä¸ªæ¥æº
                end_idx = min(start_idx + 5, len(all_sources))
                
                for j in range(start_idx, end_idx):
                    if j < len(all_sources):
                        source = all_sources[j]
                        sources_for_this_result.append(f"[{j+1}] {source.get('title', '')} - {source.get('value', '')}")
                
                if sources_for_this_result:
                    enhanced_summary += f"\n\n**Available sources for this section:**\n" + "\n".join(sources_for_this_result)
            
            enhanced_summaries.append(enhanced_summary)
        
        # å¦‚æœæ²¡æœ‰ç ”ç©¶ç»“æœä½†æœ‰æ¥æºï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬summary
        if not enhanced_summaries and all_sources:
            basic_summary = "## Available Research Sources\n\n"
            for i, source in enumerate(all_sources):
                basic_summary += f"[{i+1}] {source.get('title', '')} - {source.get('value', '')}\n"
            enhanced_summaries.append(basic_summary)
        
        formatted_prompt = answer_instructions.format(
            current_date=current_date,
            research_topic=get_research_topic(state.get("messages", [])),
            summaries="\n---\n\n".join(enhanced_summaries) if enhanced_summaries else "No research results available.",
        )
        
        print(f"ğŸ“ å¢å¼ºçš„summariesåŒ…å« {len(enhanced_summaries)} ä¸ªéƒ¨åˆ†")
        print(f"ğŸ”— æ€»å…± {len(all_sources)} ä¸ªå¯ç”¨æ¥æº")

        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"ğŸ”¥ finalize_answer: å‡†å¤‡è°ƒç”¨LLMæœåŠ¡")
        print(f"ğŸ”¥ formatted_prompté•¿åº¦: {len(formatted_prompt)}")
        print(f"ğŸ”¥ ä½¿ç”¨æ¨¡å‹: {reasoning_model}")
        
        # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ - ä½¿ç”¨æ­£ç¡®çš„LLMServiceæ¥å£
        result = await llm_service.generate(
            prompt=formatted_prompt,
            model=reasoning_model,
            task_type=LLMTaskType.ANSWER_GENERATION,
            temperature=0.3,  # ç¨å¾®æé«˜ä¸€ç‚¹creativity
            max_tokens=8000   # å¢åŠ tokené™åˆ¶ç¡®ä¿å®Œæ•´è¾“å‡º
        )

        # æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºLLMç”Ÿæˆçš„åŸå§‹å†…å®¹
        print(f"ğŸ¤– LLMç”Ÿæˆçš„åŸå§‹å›å¤å‰200å­—ç¬¦:")
        print(f"   {result.content[:200] if result.content else 'None'}...")
        
        print(f"ğŸ“š çŠ¶æ€ä¸­çš„æ‰€æœ‰æ¥æº:")
        for i, source in enumerate(state.get("sources_gathered", [])):
            print(f"   {i+1}. {source.get('short_url')} -> {source.get('value', '')[:50]}...")
            print(f"      æ ‡é¢˜: {source.get('title', '')}")
        
        # æ”¹è¿›çš„URLæ›¿æ¢é€»è¾‘
        import re
        unique_sources = []
        all_sources = state.get("sources_gathered", [])
        
        print(f"ğŸ”„ å¼€å§‹URLæ›¿æ¢å¤„ç†ï¼Œå…±æœ‰ {len(all_sources)} ä¸ªæ¥æº")
        
        # ç­–ç•¥1: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾å¹¶æ›¿æ¢æ‰€æœ‰å¼•ç”¨æ ¼å¼
        import re
        
        print(f"ğŸ” å¼€å§‹å¤„ç†å„ç§å¼•ç”¨æ ¼å¼...")
        print(f"ğŸ“ åŸå§‹å†…å®¹ç‰‡æ®µ: {result.content[:200]}...")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨æ ¼å¼ï¼ˆå•ä¸ªå’Œç»„åˆï¼‰
        citation_pattern = r'\[(\d+(?:\s*,\s*\d+)*)\]'
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„å¼•ç”¨
        matches = list(re.finditer(citation_pattern, result.content))
        print(f"ğŸ” å‘ç° {len(matches)} ä¸ªå¼•ç”¨æ ¼å¼")
        
        # ä»åå¾€å‰æ›¿æ¢ï¼Œé¿å…ä½ç½®åç§»é—®é¢˜
        for match in reversed(matches):
            full_match = match.group(0)  # å®Œæ•´çš„åŒ¹é…ï¼Œå¦‚ [1] æˆ– [1, 2, 3]
            ref_numbers_str = match.group(1)  # æ•°å­—éƒ¨åˆ†ï¼Œå¦‚ "1" æˆ– "1, 2, 3"
            
            print(f"ğŸ” å¤„ç†å¼•ç”¨: {full_match} (æ•°å­—: {ref_numbers_str})")
            
            # è§£ææ‰€æœ‰æ•°å­—
            ref_numbers = [int(n.strip()) for n in ref_numbers_str.split(',')]
            print(f"ğŸ“ å¼•ç”¨æ•°å­—: {ref_numbers}")
            
            # åˆ›å»ºå¯¹åº”çš„é“¾æ¥
            links = []
            for ref_num in ref_numbers:
                if ref_num <= len(all_sources) and ref_num > 0:
                    source = all_sources[ref_num - 1]  # è½¬æ¢ä¸º0ç´¢å¼•
                    title = source.get("title", "")
                    real_url = source.get("value", "")
                    
                    if title and title.strip():
                        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤å¯èƒ½çš„åŸŸååç¼€
                        clean_title = title.replace(".com", "").replace(".cn", "").replace(".net", "").replace(".org", "")
                        if not clean_title:
                            clean_title = title
                        link_text = clean_title
                    else:
                        link_text = f"æ¥æº{ref_num}"
                    
                    if real_url:
                        links.append(f"[{link_text}]({real_url})")
                        if source not in unique_sources:
                            unique_sources.append(source)
                else:
                    print(f"âš ï¸ å¼•ç”¨æ•°å­— {ref_num} è¶…å‡ºæ¥æºèŒƒå›´ (æœ€å¤§: {len(all_sources)})")
            
            if links:
                # ç”¨é“¾æ¥æ›¿æ¢å¼•ç”¨
                replacement = " ".join(links) if len(links) > 1 else links[0]
                print(f"âœ… æ›¿æ¢å¼•ç”¨ {full_match} -> {replacement[:100]}...")
                
                # ä½¿ç”¨matchçš„ä½ç½®ä¿¡æ¯è¿›è¡Œç²¾ç¡®æ›¿æ¢
                start, end = match.span()
                result.content = result.content[:start] + replacement + result.content[end:]
        
        print(f"ğŸ”„ æ€»å…±å¤„ç†äº† {len(matches)} ä¸ªå¼•ç”¨ï¼Œä½¿ç”¨äº† {len(unique_sources)} ä¸ªæ¥æº")
        
        # ç­–ç•¥2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¼•ç”¨ï¼Œå°è¯•ä¿®å¤LLMå¯èƒ½åˆ›å»ºçš„é”™è¯¯é“¾æ¥
        if len(unique_sources) == 0 and result.content:
            print("ğŸ”§ æœªæ‰¾åˆ°æ ‡å‡†å¼•ç”¨æ ¼å¼ï¼Œå°è¯•ä¿®å¤å¯èƒ½çš„é”™è¯¯é“¾æ¥...")
            
            # æŸ¥æ‰¾å¯èƒ½çš„markdowné“¾æ¥æ ¼å¼
            markdown_links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', result.content)
            print(f"ğŸ” å‘ç° {len(markdown_links)} ä¸ªmarkdowné“¾æ¥")
            
            for link_text, link_url in markdown_links:
                print(f"ğŸ” æ£€æŸ¥é“¾æ¥: [{link_text}]({link_url})")
                
                # å¦‚æœé“¾æ¥åŒ…å«localhostæˆ–è€…æ˜¯æ— æ•ˆçš„ï¼Œå°è¯•æ›¿æ¢
                if 'localhost' in link_url or link_url.startswith('http://localhost') or link_url == '#':
                    print(f"ğŸš¨ å‘ç°æ— æ•ˆé“¾æ¥: {link_url}")
                    
                    # å°è¯•æ‰¾åˆ°æœ€åˆé€‚çš„çœŸå®URLæ¥æ›¿æ¢
                    if all_sources:
                        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„çœŸå®URL
                        replacement_source = all_sources[0]
                        real_url = replacement_source.get("value", "")
                        if real_url:
                            print(f"ğŸ”„ æ›¿æ¢æ— æ•ˆé“¾æ¥ä¸º: {real_url[:50]}...")
                            # ä¿æŒåŸæ¥çš„é“¾æ¥æ–‡æœ¬ï¼Œåªæ›¿æ¢URL
                            result.content = result.content.replace(
                                f"[{link_text}]({link_url})", 
                                f"[{link_text}]({real_url})"
                            )
                            if replacement_source not in unique_sources:
                                unique_sources.append(replacement_source)
        
        # ç­–ç•¥3: å¦‚æœä»ç„¶æ²¡æœ‰å¼•ç”¨ï¼Œä½†æœ‰æ¥æºï¼Œåˆ™åœ¨ç­”æ¡ˆæœ«å°¾æ·»åŠ æ¥æºåˆ—è¡¨
        if len(unique_sources) == 0 and all_sources:
            print("ğŸ“š æ·»åŠ æ¥æºåˆ—è¡¨åˆ°ç­”æ¡ˆæœ«å°¾...")
            sources_section = "\n\n## å‚è€ƒæ¥æº\n\n"
            for i, source in enumerate(all_sources[:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ªæ¥æº
                real_url = source.get("value", "")
                title = source.get("title", "")
                
                if real_url:
                    # åˆ›å»ºæ›´å¥½çš„é“¾æ¥æ–‡æœ¬
                    if title and title.strip():
                        # æ¸…ç†æ ‡é¢˜
                        clean_title = title.replace(".com", "").replace(".cn", "").replace(".net", "").replace(".org", "")
                        if not clean_title:
                            clean_title = title
                        link_text = clean_title
                    else:
                        link_text = f"æ¥æº{i+1}"
                    
                    sources_section += f"{i+1}. [{link_text}]({real_url})\n"
                    unique_sources.append(source)
            
            result.content += sources_section
            print(f"âœ… æ·»åŠ äº† {len(unique_sources)} ä¸ªæ¥æºåˆ°ç­”æ¡ˆæœ«å°¾")

        print(f"âœ… ç”Ÿæˆçš„å›å¤é•¿åº¦: {len(result.content) if result.content else 0}")
        print(f"ğŸ”— å¤„ç†çš„æ¥æºæ•°é‡: {len(unique_sources)}")
        print(f"ğŸ“ æ›¿æ¢åçš„å›å¤å‰200å­—ç¬¦:")
        print(f"   {result.content[:200] if result.content else 'None'}...")
        
        # ç¡®ä¿AIMessageæœ‰ä¸€ä¸ªæœ‰æ•ˆçš„ID
        ai_message = AIMessage(
            content=result.content if result.content else "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›å¤",
            id=f"ai-{int(__import__('time').time() * 1000)}"
        )
        print(f"ğŸ“¤ è¿”å›æ¶ˆæ¯ID: {ai_message.id}")
        
        return {
            "messages": [ai_message],
            "sources_gathered": unique_sources,
        }
    except Exception as e:
        print(f"Error in finalize_answer: {e}")
        # è¿”å›é”™è¯¯æ¶ˆæ¯
        ai_message = AIMessage(
            content=f"æŠ±æ­‰ï¼Œåœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶é‡åˆ°é”™è¯¯: {str(e)}",
            id=f"ai-{int(__import__('time').time() * 1000)}"
        )
        return {
            "messages": [ai_message],
            "sources_gathered": [],
        }


def ensure_state_defaults(state: OverallState) -> OverallState:
    """ç¡®ä¿çŠ¶æ€åŒ…å«æ‰€æœ‰å¿…éœ€çš„é»˜è®¤å€¼"""
    defaults = {
        "search_query": [],
        "web_research_result": [],
        "sources_gathered": [],
        "research_loop_count": 0,
        "search_provider": "google",
        "llm_provider": "",  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä½¿ç”¨ç³»ç»Ÿé»˜è®¤é…ç½®
        "is_sufficient": False,
        "follow_up_queries": [],
        "knowledge_gap": "",
        "number_of_ran_queries": 0,
        "messages": [],
    }
    
    for key, default_value in defaults.items():
        if key not in state:
            state[key] = default_value
    
    return state


# Create our Agent Graph
builder = StateGraph(OverallState, config_schema=Configuration)

# Define the nodes we will cycle between
builder.add_node("ensure_defaults", ensure_state_defaults)
builder.add_node("generate_query", generate_query)
builder.add_node("web_research", web_research)
builder.add_node("reflection", reflection)
builder.add_node("finalize_answer", finalize_answer)

# Set the entrypoint as `ensure_defaults`
# This means that this node is the first one called
builder.add_edge(START, "ensure_defaults")
builder.add_edge("ensure_defaults", "generate_query")
# Add conditional edge to continue with search queries in a parallel branch
builder.add_conditional_edges(
    "generate_query", continue_to_web_research, ["web_research"]
)
# Reflect on the web research
builder.add_edge("web_research", "reflection")
# Evaluate the research
builder.add_conditional_edges(
    "reflection", evaluate_research, ["web_research", "finalize_answer"]
)
# Finalize the answer
builder.add_edge("finalize_answer", END)

graph = builder.compile(name="pro-search-agent")
